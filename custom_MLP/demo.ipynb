{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f344bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing embeddings...\n",
      "Epoch0: 1002/3000(33.40%)\n",
      "Epoch1: 1526/3000(50.87%)\n",
      "Epoch2: 1772/3000(59.07%)\n",
      "Epoch3: 1975/3000(65.83%)\n",
      "Epoch4: 2082/3000(69.40%)\n",
      "Epoch5: 2178/3000(72.60%)\n",
      "Epoch6: 2228/3000(74.27%)\n",
      "Epoch7: 2258/3000(75.27%)\n",
      "Epoch8: 2301/3000(76.70%)\n",
      "Epoch9: 2317/3000(77.23%)\n",
      "Epoch10: 2355/3000(78.50%)\n",
      "Epoch11: 2360/3000(78.67%)\n",
      "Epoch12: 2382/3000(79.40%)\n",
      "Epoch13: 2394/3000(79.80%)\n",
      "Epoch14: 2400/3000(80.00%)\n",
      "Epoch15: 2429/3000(80.97%)\n",
      "Epoch16: 2428/3000(80.93%)\n",
      "Epoch17: 2443/3000(81.43%)\n",
      "Epoch18: 2446/3000(81.53%)\n",
      "Epoch19: 2452/3000(81.73%)\n",
      "Epoch20: 2448/3000(81.60%)\n",
      "Epoch21: 2466/3000(82.20%)\n",
      "Epoch22: 2458/3000(81.93%)\n",
      "Epoch23: 2470/3000(82.33%)\n",
      "Epoch24: 2469/3000(82.30%)\n",
      "Epoch25: 2475/3000(82.50%)\n",
      "Epoch26: 2483/3000(82.77%)\n",
      "Epoch27: 2489/3000(82.97%)\n",
      "Epoch28: 2495/3000(83.17%)\n",
      "Epoch29: 2495/3000(83.17%)\n",
      "Epoch30: 2504/3000(83.47%)\n",
      "Epoch31: 2504/3000(83.47%)\n",
      "Epoch32: 2506/3000(83.53%)\n",
      "Epoch33: 2518/3000(83.93%)\n",
      "Epoch34: 2518/3000(83.93%)\n",
      "Epoch35: 2513/3000(83.77%)\n",
      "Epoch36: 2516/3000(83.87%)\n",
      "Epoch37: 2516/3000(83.87%)\n",
      "Epoch38: 2526/3000(84.20%)\n",
      "Epoch39: 2528/3000(84.27%)\n",
      "Epoch40: 2528/3000(84.27%)\n",
      "Epoch41: 2522/3000(84.07%)\n",
      "Epoch42: 2525/3000(84.17%)\n",
      "Epoch43: 2529/3000(84.30%)\n",
      "Epoch44: 2531/3000(84.37%)\n",
      "Epoch45: 2535/3000(84.50%)\n",
      "Epoch46: 2528/3000(84.27%)\n",
      "Epoch47: 2530/3000(84.33%)\n",
      "Epoch48: 2539/3000(84.63%)\n",
      "Epoch49: 2538/3000(84.60%)\n",
      "Epoch50: 2533/3000(84.43%)\n",
      "Epoch51: 2536/3000(84.53%)\n",
      "Epoch52: 2538/3000(84.60%)\n",
      "Epoch53: 2539/3000(84.63%)\n",
      "Epoch54: 2542/3000(84.73%)\n",
      "Epoch55: 2536/3000(84.53%)\n",
      "Epoch56: 2534/3000(84.47%)\n",
      "Epoch57: 2539/3000(84.63%)\n",
      "Epoch58: 2539/3000(84.63%)\n",
      "Epoch59: 2540/3000(84.67%)\n",
      "Epoch60: 2543/3000(84.77%)\n",
      "Epoch61: 2538/3000(84.60%)\n",
      "Epoch62: 2541/3000(84.70%)\n",
      "Epoch63: 2540/3000(84.67%)\n",
      "Epoch64: 2542/3000(84.73%)\n",
      "Epoch65: 2547/3000(84.90%)\n",
      "Epoch66: 2544/3000(84.80%)\n",
      "Epoch67: 2547/3000(84.90%)\n",
      "Epoch68: 2550/3000(85.00%)\n",
      "Epoch69: 2549/3000(84.97%)\n",
      "Epoch70: 2544/3000(84.80%)\n",
      "Epoch71: 2547/3000(84.90%)\n",
      "Epoch72: 2552/3000(85.07%)\n",
      "Epoch73: 2548/3000(84.93%)\n",
      "Epoch74: 2554/3000(85.13%)\n",
      "Epoch75: 2551/3000(85.03%)\n",
      "Epoch76: 2553/3000(85.10%)\n",
      "Epoch77: 2552/3000(85.07%)\n",
      "Epoch78: 2554/3000(85.13%)\n",
      "Epoch79: 2553/3000(85.10%)\n",
      "Epoch80: 2556/3000(85.20%)\n",
      "Epoch81: 2554/3000(85.13%)\n",
      "Epoch82: 2549/3000(84.97%)\n",
      "Epoch83: 2561/3000(85.37%)\n",
      "Epoch84: 2555/3000(85.17%)\n",
      "Epoch85: 2562/3000(85.40%)\n",
      "Epoch86: 2552/3000(85.07%)\n",
      "Epoch87: 2558/3000(85.27%)\n",
      "Epoch88: 2558/3000(85.27%)\n",
      "Epoch89: 2558/3000(85.27%)\n",
      "Epoch90: 2558/3000(85.27%)\n",
      "Epoch91: 2562/3000(85.40%)\n",
      "Epoch92: 2560/3000(85.33%)\n",
      "Epoch93: 2565/3000(85.50%)\n",
      "Epoch94: 2561/3000(85.37%)\n",
      "Epoch95: 2566/3000(85.53%)\n",
      "Epoch96: 2565/3000(85.50%)\n",
      "Epoch97: 2563/3000(85.43%)\n",
      "Epoch98: 2562/3000(85.40%)\n",
      "Epoch99: 2562/3000(85.40%)\n",
      "Epoch100: 2558/3000(85.27%)\n",
      "Epoch101: 2563/3000(85.43%)\n",
      "Epoch102: 2567/3000(85.57%)\n",
      "Epoch103: 2561/3000(85.37%)\n",
      "Epoch104: 2568/3000(85.60%)\n",
      "Epoch105: 2567/3000(85.57%)\n",
      "Epoch106: 2566/3000(85.53%)\n",
      "Epoch107: 2565/3000(85.50%)\n",
      "Epoch108: 2562/3000(85.40%)\n",
      "Epoch109: 2565/3000(85.50%)\n",
      "Epoch110: 2570/3000(85.67%)\n",
      "Epoch111: 2568/3000(85.60%)\n",
      "Epoch112: 2569/3000(85.63%)\n",
      "Epoch113: 2570/3000(85.67%)\n",
      "Epoch114: 2569/3000(85.63%)\n",
      "Epoch115: 2568/3000(85.60%)\n",
      "Epoch116: 2569/3000(85.63%)\n",
      "Epoch117: 2570/3000(85.67%)\n",
      "Epoch118: 2573/3000(85.77%)\n",
      "Epoch119: 2575/3000(85.83%)\n",
      "Epoch120: 2565/3000(85.50%)\n",
      "Epoch121: 2569/3000(85.63%)\n",
      "Epoch122: 2571/3000(85.70%)\n",
      "Epoch123: 2573/3000(85.77%)\n",
      "Epoch124: 2566/3000(85.53%)\n",
      "Epoch125: 2571/3000(85.70%)\n",
      "Epoch126: 2573/3000(85.77%)\n",
      "Epoch127: 2572/3000(85.73%)\n",
      "Epoch128: 2582/3000(86.07%)\n",
      "Epoch129: 2576/3000(85.87%)\n",
      "Epoch130: 2574/3000(85.80%)\n",
      "Epoch131: 2574/3000(85.80%)\n",
      "Epoch132: 2577/3000(85.90%)\n",
      "Epoch133: 2575/3000(85.83%)\n",
      "Epoch134: 2577/3000(85.90%)\n",
      "Epoch135: 2578/3000(85.93%)\n",
      "Epoch136: 2576/3000(85.87%)\n",
      "Epoch137: 2579/3000(85.97%)\n",
      "Early stopping due to no improvement in validation accuracy for 10 epochs\n",
      "Training complete\n",
      "Iteration 1, loss = 0.54763873\n",
      "Iteration 2, loss = 0.35969641\n",
      "Iteration 3, loss = 0.32538328\n",
      "Iteration 4, loss = 0.29917917\n",
      "Iteration 5, loss = 0.27055183\n",
      "Iteration 6, loss = 0.25251775\n",
      "Iteration 7, loss = 0.23852577\n",
      "Iteration 8, loss = 0.20437280\n",
      "Iteration 9, loss = 0.18452524\n",
      "Iteration 10, loss = 0.17141943\n",
      "Iteration 11, loss = 0.15522342\n",
      "Iteration 12, loss = 0.13946996\n",
      "Iteration 13, loss = 0.12191516\n",
      "Iteration 14, loss = 0.11906161\n",
      "Iteration 15, loss = 0.10377595\n",
      "Iteration 16, loss = 0.10853485\n",
      "Iteration 17, loss = 0.08752092\n",
      "Iteration 18, loss = 0.06832255\n",
      "Iteration 19, loss = 0.07726257\n",
      "Iteration 20, loss = 0.07448349\n",
      "Iteration 21, loss = 0.07349718\n",
      "Iteration 22, loss = 0.06371714\n",
      "Iteration 23, loss = 0.06346077\n",
      "Iteration 24, loss = 0.05664982\n",
      "Iteration 25, loss = 0.04980090\n",
      "Iteration 26, loss = 0.06161557\n",
      "Iteration 27, loss = 0.03169497\n",
      "Iteration 28, loss = 0.05151835\n",
      "Iteration 29, loss = 0.05795083\n",
      "Iteration 30, loss = 0.04327340\n",
      "Iteration 31, loss = 0.05754259\n",
      "Iteration 32, loss = 0.05692780\n",
      "Iteration 33, loss = 0.02876282\n",
      "Iteration 34, loss = 0.04156241\n",
      "Iteration 35, loss = 0.03725738\n",
      "Iteration 36, loss = 0.02494667\n",
      "Iteration 37, loss = 0.04436587\n",
      "Iteration 38, loss = 0.04150810\n",
      "Iteration 39, loss = 0.04415486\n",
      "Iteration 40, loss = 0.04057727\n",
      "Iteration 41, loss = 0.03157818\n",
      "Iteration 42, loss = 0.02862404\n",
      "Iteration 43, loss = 0.03594786\n",
      "Iteration 44, loss = 0.02253370\n",
      "Iteration 45, loss = 0.02920069\n",
      "Iteration 46, loss = 0.02084677\n",
      "Iteration 47, loss = 0.02779778\n",
      "Iteration 48, loss = 0.04038504\n",
      "Iteration 49, loss = 0.02665333\n",
      "Iteration 50, loss = 0.01616682\n",
      "Iteration 51, loss = 0.01162822\n",
      "Iteration 52, loss = 0.05913466\n",
      "Iteration 53, loss = 0.06414361\n",
      "Iteration 54, loss = 0.04956138\n",
      "Iteration 55, loss = 0.03695202\n",
      "Iteration 56, loss = 0.02984814\n",
      "Iteration 57, loss = 0.01992303\n",
      "Iteration 58, loss = 0.03289973\n",
      "Iteration 59, loss = 0.03567045\n",
      "Iteration 60, loss = 0.03595667\n",
      "Iteration 61, loss = 0.02538835\n",
      "Iteration 62, loss = 0.03170636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "baseline Accuracy: 0.852\n",
      "Test Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier as sklearn_mlp\n",
    "from custom_mlp import mlpClassifier as mlp\n",
    "from demo import runner\n",
    "CONFIGS = {\n",
    "    \"data_path\": \"../project_data/t4sa_data.csv\",\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "runner(CONFIGS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
