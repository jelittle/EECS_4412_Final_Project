{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "edf896d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10aa1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "\n",
    "TWEETS_PATH= \"../data/raw_tweets_text.csv\"\n",
    "SENTIMENT_PATH=\"../data/t4sa_text_sentiment.tsv\"\n",
    "\n",
    "#load data\n",
    "tweets_df = pd.read_csv(TWEETS_PATH, encoding='latin-1', header=0)\n",
    "sentiment_df= pd.read_csv(SENTIMENT_PATH, sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05e454",
   "metadata": {},
   "source": [
    "# Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "688aa746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3452663, 2)\n",
      "(1179957, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.shape)\n",
    "print(sentiment_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "78c6350f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id  \\\n",
      "0  758014713804587008   \n",
      "1  758014717990428672   \n",
      "2  758014646716665857   \n",
      "3  758014655071526912   \n",
      "4  758014642526429184   \n",
      "\n",
      "                                                                                                                                             text  \n",
      "0  RT @polarcomic: And surprise! the #RegularShow #38 has a short story written and illustrated (and even lettered) by me. https://t.co/kCctJpâ¦  \n",
      "1                                             RT @SweetBabyBellB: My unproblematic fav who knows Bellarke is fucking real https://t.co/A9RK5b0Hfm  \n",
      "2                                                                    RT @WhyLarryIsReal: I mean we know harry isn't human https://t.co/fW2TEwSHEq  \n",
      "3          RT @Eastbay: She's ready, resilient, and on our latest cover. Snag a copy to find out more about @crysdunn_19. https://t.co/j4JwiEgCmd  \n",
      "4                             RT @SheeeRatchet: find someone who loves you as much as Pikachu loves his bottle of ketchup https://t.co/pbIoDo9bfy  \n",
      "                 TWID       NEG       NEU       POS\n",
      "0  768096868504969216  0.049398  0.861395  0.089207\n",
      "1  768097237620490241  0.028733  0.929554  0.041713\n",
      "2  768097619281227776  0.006598  0.046810  0.946591\n",
      "3  768097619285536768  0.032333  0.850945  0.116722\n",
      "4  768097627686604801  0.008090  0.042331  0.949579\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.head())\n",
    "print(sentiment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07eaedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id\n",
      "count  3.452663e+06\n",
      "mean   7.865426e+17\n",
      "std    1.381574e+16\n",
      "min    7.580146e+17\n",
      "25%    7.695707e+17\n",
      "50%    7.862700e+17\n",
      "75%    7.993751e+17\n",
      "max    8.046194e+17\n",
      "               TWID           NEG           NEU           POS\n",
      "count  1.179957e+06  1.179957e+06  1.179957e+06  1.179957e+06\n",
      "mean   7.860716e+17  1.214643e-01  5.272504e-01  3.512854e-01\n",
      "std    1.386547e+16  2.489799e-01  3.953345e-01  3.879292e-01\n",
      "min    7.680969e+17  2.930239e-14  2.250815e-03  2.441870e-14\n",
      "25%    7.692905e+17  1.118029e-02  8.237851e-02  7.404817e-02\n",
      "50%    7.839379e+17  1.924086e-02  8.548171e-01  1.057651e-01\n",
      "75%    7.996407e+17  3.546559e-02  8.904971e-01  8.860867e-01\n",
      "max    8.046194e+17  9.939882e-01  1.000000e+00  9.965788e-01\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.describe())\n",
    "print(sentiment_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd536d",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8112fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate amounts in tweets_df:\n",
      "0\n",
      "Duplicate amounts in sentiment_df:\n",
      "0\n",
      "Missing values in tweets_df:\n",
      "id      0\n",
      "text    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in sentiment_df:\n",
      "TWID    0\n",
      "NEG     0\n",
      "NEU     0\n",
      "POS     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Duplicate amounts in tweets_df:\")\n",
    "print(tweets_df['id'].duplicated().sum()) # There are no duplicates in either dataset\n",
    "\n",
    "print(\"Duplicate amounts in sentiment_df:\")\n",
    "print(sentiment_df.duplicated().sum())\n",
    "\n",
    "# Check for missing values in tweets_df\n",
    "print(\"Missing values in tweets_df:\")\n",
    "print(tweets_df.isna().sum())\n",
    "\n",
    "# Check for missing values in sentiment_df\n",
    "print(\"\\nMissing values in sentiment_df:\")\n",
    "print(sentiment_df.isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d79d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(tweets_df, sentiment_df, left_on='id', right_on='TWID')\n",
    "merged_df = merged_df.drop(columns=['TWID']) # since its alr in id\n",
    "\n",
    "main_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "423c7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions to help extract data from the columns\n",
    "\n",
    "def extract_username_from_text(text):     # Extracts the username from retweets (e.g., \"RT @user:\").\n",
    "    match = re.search(r'^RT @([^\\s:]+):', text)\n",
    "    return match.group(1) if match else None\n",
    "     \n",
    "def extract_links_from_text(text): # Extracts URLs from tweet text.\n",
    "    urls = re.findall(r'https?://\\S+', text)\n",
    "    if not urls:\n",
    "        return None\n",
    "    return urls[0] if len(urls) == 1 else urls\n",
    "           \n",
    "def extract_hashtags_from_text(text): # Extract hashtags from the tweet text\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "    if not hashtags:\n",
    "        return None\n",
    "    return hashtags[0] if len(hashtags) == 1 else hashtags\n",
    "    \n",
    "def extract_mentions_from_text(text): # Extract mentions from the tweet text\n",
    "    cleaned_text = re.sub(r'^RT @[^\\s:]+: ', '', text) # Remove the initial retweet username (e.g., \"RT @user:\")\n",
    "    mentions = re.findall(r'@\\w+', cleaned_text)\n",
    "    if not mentions:\n",
    "        return None\n",
    "    return mentions[0] if len(mentions) == 1 else mentions\n",
    "\n",
    "\n",
    "\n",
    "def clean_tweet_text(text: str):\n",
    "    \"\"\"\n",
    "    Cleans the tweet text for EDA by removing noise such as:\n",
    "    - Retweet prefixes (RT @user:)\n",
    "    - URLs\n",
    "    - HTML entities (e.g., &amp;)\n",
    "    - Extra whitespace\n",
    "    - Remove mentions\n",
    "    \"\"\"\n",
    "    # Remove retweet header\n",
    "    text = re.sub(r'^RT @[^\\s:]+: ', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    # Remove URLs including malformed/truncated ones (e.g., \"httpsâ\")\n",
    "    text = re.sub(r'https?\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML entities like &amp;\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    \n",
    "    # Remove extra spaces and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #remove metions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24524d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33musername\u001b[39m\u001b[33m'\u001b[39m] = main_df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(extract_username_from_text)\n\u001b[32m      4\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33murls\u001b[39m\u001b[33m'\u001b[39m] = main_df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(extract_links_from_text)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mmain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tweet_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33mhashtags\u001b[39m\u001b[33m'\u001b[39m] = main_df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(extract_hashtags_from_text)\n\u001b[32m      7\u001b[39m main_df[\u001b[33m'\u001b[39m\u001b[33mmentions\u001b[39m\u001b[33m'\u001b[39m] = main_df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].apply(extract_mentions_from_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/EECS_4412_Final_Project/.venv/lib64/python3.13/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/EECS_4412_Final_Project/.venv/lib64/python3.13/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/EECS_4412_Final_Project/.venv/lib64/python3.13/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/EECS_4412_Final_Project/.venv/lib64/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding/EECS_4412_Final_Project/.venv/lib64/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mclean_tweet_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     38\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m^RT @[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms:]+: \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Remove URLs\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Remove URLs including malformed/truncated ones (e.g., \"httpsâ\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m text = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps?\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mS+|www\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mS+\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Remove HTML entities like &amp;\u001b[39;00m\n\u001b[32m     45\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m&\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+;\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/re/__init__.py:208\u001b[39m, in \u001b[36msub\u001b[39m\u001b[34m(pattern, repl, string, count, flags, *args)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    203\u001b[39m     warnings.warn(\n\u001b[32m    204\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    205\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    206\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Start adding extra columns that might help us with visualizations\n",
    "main_df['is_retweet'] = main_df['text'].str.startswith('RT ')\n",
    "main_df['username'] = main_df['text'].apply(extract_username_from_text)\n",
    "main_df['urls'] = main_df['text'].apply(extract_links_from_text)\n",
    "main_df['cleaned_text'] = main_df['text'].apply(clean_tweet_text)\n",
    "main_df['hashtags'] = main_df['text'].apply(extract_hashtags_from_text)\n",
    "main_df['mentions'] = main_df['text'].apply(extract_mentions_from_text)\n",
    "\n",
    "main_df\n",
    "# filter to rows where cleaned_text contains 'https'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac221506",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_DF=main_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007bb773",
   "metadata": {},
   "source": [
    "#for easier analysis, create a single sentiment column, loses some info but easier to visualize\n",
    "\n",
    "we also drop values with low confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba5a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>NEG</th>\n",
       "      <th>NEU</th>\n",
       "      <th>POS</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>username</th>\n",
       "      <th>urls</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>dominant_sentiment</th>\n",
       "      <th>dominant_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>text_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768097627686604801</td>\n",
       "      <td>Josh Jenkins is looking forward to TAB Breeders Crown Super Sunday https://t.co/antImqAo4Y https://t.co/ejnA78Sks0</td>\n",
       "      <td>0.008090</td>\n",
       "      <td>0.042331</td>\n",
       "      <td>0.949579</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://t.co/antImqAo4Y, https://t.co/ejnA78Sks0]</td>\n",
       "      <td>Josh Jenkins is looking forward to TAB Breeders Crown Super Sunday</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.949579</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>5.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768097631864102912</td>\n",
       "      <td>RT @2pmthailfans: [Pic] Nichkhun from krjeong86's IG https://t.co/5gcAcu9by7</td>\n",
       "      <td>0.014644</td>\n",
       "      <td>0.926557</td>\n",
       "      <td>0.058800</td>\n",
       "      <td>True</td>\n",
       "      <td>2pmthailfans</td>\n",
       "      <td>https://t.co/5gcAcu9by7</td>\n",
       "      <td>[Pic] Nichkhun from krjeong86's IG</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NEU</td>\n",
       "      <td>0.926557</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768097640278089729</td>\n",
       "      <td>RT @MianUsmanJaved: Congratulations Pakistan on becoming #No1TestTeam in the world against all odds! #JI_PakZindabadRallies https://t.co/1oâ¦</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>True</td>\n",
       "      <td>MianUsmanJaved</td>\n",
       "      <td>https://t.co/1oâ¦</td>\n",
       "      <td>Congratulations Pakistan on becoming #No1TestTeam in the world against all odds! #JI_PakZindabadRallies</td>\n",
       "      <td>[#No1TestTeam, #JI_PakZindabadRallies]</td>\n",
       "      <td>None</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>12</td>\n",
       "      <td>103</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768097627695042560</td>\n",
       "      <td>RT @PEPalerts: This September, @YESmag is taking you to Maine Mendozaâs surprise thanksgiving party she threw for her fans! https://t.co/oXâ¦</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.974948</td>\n",
       "      <td>True</td>\n",
       "      <td>PEPalerts</td>\n",
       "      <td>https://t.co/oXâ¦</td>\n",
       "      <td>This September,  is taking you to Maine Mendozaâs surprise thanksgiving party she threw for her fans!</td>\n",
       "      <td>None</td>\n",
       "      <td>@YESmag</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.974948</td>\n",
       "      <td>16</td>\n",
       "      <td>103</td>\n",
       "      <td>5.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>768097661237026816</td>\n",
       "      <td>RT @david_gaibis: Newly painted walls, thanks a million to our custodial painters this summer.  Great job ladies!!!#EC_proud https://t.co/â¦</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.035123</td>\n",
       "      <td>0.956987</td>\n",
       "      <td>True</td>\n",
       "      <td>david_gaibis</td>\n",
       "      <td>https://t.co/â¦</td>\n",
       "      <td>Newly painted walls, thanks a million to our custodial painters this summer. Great job ladies!!!#EC_proud</td>\n",
       "      <td>#EC_proud</td>\n",
       "      <td>None</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.956987</td>\n",
       "      <td>15</td>\n",
       "      <td>105</td>\n",
       "      <td>6.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179951</th>\n",
       "      <td>804618086913437696</td>\n",
       "      <td>#antsmasher I smashed  7 ants in this awesome game!!!hjfjfi https://t.co/40RsxZpxZq https://t.co/wC0FTrO8l9</td>\n",
       "      <td>0.012427</td>\n",
       "      <td>0.030774</td>\n",
       "      <td>0.956799</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://t.co/40RsxZpxZq, https://t.co/wC0FTrO8l9]</td>\n",
       "      <td>#antsmasher I smashed 7 ants in this awesome game!!!hjfjfi</td>\n",
       "      <td>#antsmasher</td>\n",
       "      <td>None</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.956799</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>5.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179952</th>\n",
       "      <td>804618351179874304</td>\n",
       "      <td>@LizHudston @KymWyllie @Evasmiless @meanBok @linddyloo66 @Minna1971 morning girls have a wonderful #Friday https://t.co/unkV2p7JYF</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>0.968946</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>https://t.co/unkV2p7JYF</td>\n",
       "      <td>morning girls have a wonderful #Friday</td>\n",
       "      <td>#Friday</td>\n",
       "      <td>[@LizHudston, @KymWyllie, @Evasmiless, @meanBok, @linddyloo66, @Minna1971]</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.968946</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179954</th>\n",
       "      <td>804618716084391936</td>\n",
       "      <td>I am now live on webcam find me here &amp;gt;&amp;gt; https://t.co/yg0pJss4MK download our app &amp;gt;&amp;gt; here https://t.co/QMXtTx4Gcr https://t.co/qxJFN7ZO5U</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>0.927865</td>\n",
       "      <td>0.053456</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://t.co/yg0pJss4MK, https://t.co/QMXtTx4Gcr, https://t.co/qxJFN7ZO5U]</td>\n",
       "      <td>I am now live on webcam find me here download our app here</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NEU</td>\n",
       "      <td>0.927865</td>\n",
       "      <td>13</td>\n",
       "      <td>58</td>\n",
       "      <td>3.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179955</th>\n",
       "      <td>804618934158757889</td>\n",
       "      <td>Pearl Roadshow 4-piece Complete Drum Set with Cymb https://t.co/gQ2TdFKnma https://t.co/PL2FXvWRo9</td>\n",
       "      <td>0.019658</td>\n",
       "      <td>0.907034</td>\n",
       "      <td>0.073308</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://t.co/gQ2TdFKnma, https://t.co/PL2FXvWRo9]</td>\n",
       "      <td>Pearl Roadshow 4-piece Complete Drum Set with Cymb</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NEU</td>\n",
       "      <td>0.907034</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>5.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179956</th>\n",
       "      <td>804619286488682496</td>\n",
       "      <td>Bixbeat Mixtape Vol.2 is here with great artiste join the movement. 08187593720 https://t.co/Iqm1DrszLf @BixBEAT https://t.co/DOXWHP8XCQ</td>\n",
       "      <td>0.005901</td>\n",
       "      <td>0.042582</td>\n",
       "      <td>0.951517</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://t.co/Iqm1DrszLf, https://t.co/DOXWHP8XCQ]</td>\n",
       "      <td>Bixbeat Mixtape Vol.2 is here with great artiste join the movement. 08187593720</td>\n",
       "      <td>None</td>\n",
       "      <td>@BixBEAT</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.951517</td>\n",
       "      <td>12</td>\n",
       "      <td>80</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504519 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0        768097627686604801   \n",
       "1        768097631864102912   \n",
       "2        768097640278089729   \n",
       "3        768097627695042560   \n",
       "5        768097661237026816   \n",
       "...                     ...   \n",
       "1179951  804618086913437696   \n",
       "1179952  804618351179874304   \n",
       "1179954  804618716084391936   \n",
       "1179955  804618934158757889   \n",
       "1179956  804619286488682496   \n",
       "\n",
       "                                                                                                                                                         text  \\\n",
       "0                                          Josh Jenkins is looking forward to TAB Breeders Crown Super Sunday https://t.co/antImqAo4Y https://t.co/ejnA78Sks0   \n",
       "1                                                                                RT @2pmthailfans: [Pic] Nichkhun from krjeong86's IG https://t.co/5gcAcu9by7   \n",
       "2              RT @MianUsmanJaved: Congratulations Pakistan on becoming #No1TestTeam in the world against all odds! #JI_PakZindabadRallies https://t.co/1oâ¦   \n",
       "3            RT @PEPalerts: This September, @YESmag is taking you to Maine Mendozaâs surprise thanksgiving party she threw for her fans! https://t.co/oXâ¦   \n",
       "5               RT @david_gaibis: Newly painted walls, thanks a million to our custodial painters this summer.  Great job ladies!!!#EC_proud https://t.co/â¦   \n",
       "...                                                                                                                                                       ...   \n",
       "1179951                                           #antsmasher I smashed  7 ants in this awesome game!!!hjfjfi https://t.co/40RsxZpxZq https://t.co/wC0FTrO8l9   \n",
       "1179952                    @LizHudston @KymWyllie @Evasmiless @meanBok @linddyloo66 @Minna1971 morning girls have a wonderful #Friday https://t.co/unkV2p7JYF   \n",
       "1179954  I am now live on webcam find me here &gt;&gt; https://t.co/yg0pJss4MK download our app &gt;&gt; here https://t.co/QMXtTx4Gcr https://t.co/qxJFN7ZO5U   \n",
       "1179955                                                    Pearl Roadshow 4-piece Complete Drum Set with Cymb https://t.co/gQ2TdFKnma https://t.co/PL2FXvWRo9   \n",
       "1179956              Bixbeat Mixtape Vol.2 is here with great artiste join the movement. 08187593720 https://t.co/Iqm1DrszLf @BixBEAT https://t.co/DOXWHP8XCQ   \n",
       "\n",
       "              NEG       NEU       POS  is_retweet        username  \\\n",
       "0        0.008090  0.042331  0.949579       False            None   \n",
       "1        0.014644  0.926557  0.058800        True    2pmthailfans   \n",
       "2        0.004939  0.029469  0.965591        True  MianUsmanJaved   \n",
       "3        0.006389  0.018663  0.974948        True       PEPalerts   \n",
       "5        0.007890  0.035123  0.956987        True    david_gaibis   \n",
       "...           ...       ...       ...         ...             ...   \n",
       "1179951  0.012427  0.030774  0.956799       False            None   \n",
       "1179952  0.006605  0.024448  0.968946       False            None   \n",
       "1179954  0.018679  0.927865  0.053456       False            None   \n",
       "1179955  0.019658  0.907034  0.073308       False            None   \n",
       "1179956  0.005901  0.042582  0.951517       False            None   \n",
       "\n",
       "                                                                                urls  \\\n",
       "0                                 [https://t.co/antImqAo4Y, https://t.co/ejnA78Sks0]   \n",
       "1                                                            https://t.co/5gcAcu9by7   \n",
       "2                                                                 https://t.co/1oâ¦   \n",
       "3                                                                 https://t.co/oXâ¦   \n",
       "5                                                                   https://t.co/â¦   \n",
       "...                                                                              ...   \n",
       "1179951                           [https://t.co/40RsxZpxZq, https://t.co/wC0FTrO8l9]   \n",
       "1179952                                                      https://t.co/unkV2p7JYF   \n",
       "1179954  [https://t.co/yg0pJss4MK, https://t.co/QMXtTx4Gcr, https://t.co/qxJFN7ZO5U]   \n",
       "1179955                           [https://t.co/gQ2TdFKnma, https://t.co/PL2FXvWRo9]   \n",
       "1179956                           [https://t.co/Iqm1DrszLf, https://t.co/DOXWHP8XCQ]   \n",
       "\n",
       "                                                                                                      cleaned_text  \\\n",
       "0                                               Josh Jenkins is looking forward to TAB Breeders Crown Super Sunday   \n",
       "1                                                                               [Pic] Nichkhun from krjeong86's IG   \n",
       "2          Congratulations Pakistan on becoming #No1TestTeam in the world against all odds! #JI_PakZindabadRallies   \n",
       "3          This September,  is taking you to Maine Mendozaâs surprise thanksgiving party she threw for her fans!   \n",
       "5        Newly painted walls, thanks a million to our custodial painters this summer. Great job ladies!!!#EC_proud   \n",
       "...                                                                                                            ...   \n",
       "1179951                                                 #antsmasher I smashed 7 ants in this awesome game!!!hjfjfi   \n",
       "1179952                                                                     morning girls have a wonderful #Friday   \n",
       "1179954                                                 I am now live on webcam find me here download our app here   \n",
       "1179955                                                         Pearl Roadshow 4-piece Complete Drum Set with Cymb   \n",
       "1179956                           Bixbeat Mixtape Vol.2 is here with great artiste join the movement. 08187593720    \n",
       "\n",
       "                                       hashtags  \\\n",
       "0                                          None   \n",
       "1                                          None   \n",
       "2        [#No1TestTeam, #JI_PakZindabadRallies]   \n",
       "3                                          None   \n",
       "5                                     #EC_proud   \n",
       "...                                         ...   \n",
       "1179951                             #antsmasher   \n",
       "1179952                                 #Friday   \n",
       "1179954                                    None   \n",
       "1179955                                    None   \n",
       "1179956                                    None   \n",
       "\n",
       "                                                                           mentions  \\\n",
       "0                                                                              None   \n",
       "1                                                                              None   \n",
       "2                                                                              None   \n",
       "3                                                                           @YESmag   \n",
       "5                                                                              None   \n",
       "...                                                                             ...   \n",
       "1179951                                                                        None   \n",
       "1179952  [@LizHudston, @KymWyllie, @Evasmiless, @meanBok, @linddyloo66, @Minna1971]   \n",
       "1179954                                                                        None   \n",
       "1179955                                                                        None   \n",
       "1179956                                                                    @BixBEAT   \n",
       "\n",
       "        dominant_sentiment  dominant_score  word_count  text_length  \\\n",
       "0                      POS        0.949579          11           66   \n",
       "1                      NEU        0.926557           5           34   \n",
       "2                      POS        0.965591          12          103   \n",
       "3                      POS        0.974948          16          103   \n",
       "5                      POS        0.956987          15          105   \n",
       "...                    ...             ...         ...          ...   \n",
       "1179951                POS        0.956799           9           58   \n",
       "1179952                POS        0.968946           6           44   \n",
       "1179954                NEU        0.927865          13           58   \n",
       "1179955                NEU        0.907034           8           50   \n",
       "1179956                POS        0.951517          12           80   \n",
       "\n",
       "         avg_word_length  \n",
       "0               5.090909  \n",
       "1               6.000000  \n",
       "2               7.666667  \n",
       "3               5.437500  \n",
       "5               6.066667  \n",
       "...                  ...  \n",
       "1179951         5.555556  \n",
       "1179952         5.500000  \n",
       "1179954         3.538462  \n",
       "1179955         5.375000  \n",
       "1179956         5.666667  \n",
       "\n",
       "[504519 rows x 16 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.9\n",
    "cols=['NEG', 'NEU', 'POS']\n",
    "EDA_DF[cols] = EDA_DF[cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "EDA_DF['dominant_sentiment'] = EDA_DF[cols].idxmax(axis=1)   # 'NEG'/'NEU'/'POS'\n",
    "EDA_DF['dominant_score'] = EDA_DF[cols].max(axis=1)\n",
    "# drop rows with low confidence\n",
    "EDA_DF = EDA_DF[EDA_DF['dominant_score'] >= threshold].copy()\n",
    "\n",
    "#adding word and text based features\n",
    "EDA_DF['word_count'] = EDA_DF['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "EDA_DF['text_length'] = EDA_DF['cleaned_text'].str.len()\n",
    "EDA_DF['avg_word_length'] = EDA_DF['cleaned_text'].apply(lambda t: np.mean([len(w) for w in t.split()]) if t.split() else 0)\n",
    "\n",
    "\n",
    "EDA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076025f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    stop_words = stopwords.words('english')\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, min_df=5, ngram_range=(1,3)) \n",
    "#fit on entire dataset\n",
    "vectorizer.fit(EDA_DF['cleaned_text'])\n",
    "\n",
    "#transform each subset\n",
    "neg_matrix = vectorizer.transform(EDA_DF[EDA_DF['dominant_sentiment'] == 'NEG']['cleaned_text'])\n",
    "pos_matrix = vectorizer.transform(EDA_DF[EDA_DF['dominant_sentiment'] == 'POS']['cleaned_text'])\n",
    "neu_matrix = vectorizer.transform(EDA_DF[EDA_DF['dominant_sentiment'] == 'NEU']['cleaned_text'])\n",
    "\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "#  counts number of tweets containing a word(multiple words in same tweet are not counterd twice)\n",
    "neg_counts = np.asarray((neg_matrix > 0).sum(axis=0)).ravel()\n",
    "neu_counts = np.asarray((neu_matrix > 0).sum(axis=0)).ravel()\n",
    "pos_counts = np.asarray((pos_matrix > 0).sum(axis=0)).ravel()\n",
    "\n",
    "\n",
    "word_freq = pd.DataFrame({\n",
    "    'word': vocab,\n",
    "    'neg_count': neg_counts,\n",
    "    'neu_count': neu_counts,\n",
    "    'pos_count': pos_counts\n",
    "})\n",
    "\n",
    "\n",
    "word_freq['total_count'] = word_freq[['neg_count', 'neu_count', 'pos_count']].sum(axis=1)\n",
    "\n",
    "# handle ngrams\n",
    "max_n = vectorizer.ngram_range[1] \n",
    "word_freq['ngram_len'] = word_freq['word'].str.split().apply(len)\n",
    "word_freq['is_ngram'] = word_freq['ngram_len'] > 1\n",
    "\n",
    "\n",
    "# supports \n",
    "total_docs = len(EDA_DF)\n",
    "word_freq['support_neg'] = word_freq['neg_count'] / total_docs\n",
    "word_freq['support_neu'] = word_freq['neu_count'] / total_docs\n",
    "word_freq['support_pos'] = word_freq['pos_count'] / total_docs\n",
    "\n",
    "# confidence \n",
    "word_freq[['confidence_neg', 'confidence_neu', 'confidence_pos']] = (\n",
    "    word_freq[['neg_count', 'neu_count', 'pos_count']]\n",
    "    .div(word_freq['total_count'].replace(0, np.nan), axis=0)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "299dd909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>neg_count</th>\n",
       "      <th>neu_count</th>\n",
       "      <th>pos_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>ngram_len</th>\n",
       "      <th>is_ngram</th>\n",
       "      <th>support_neg</th>\n",
       "      <th>support_neu</th>\n",
       "      <th>support_pos</th>\n",
       "      <th>confidence_neg</th>\n",
       "      <th>confidence_neu</th>\n",
       "      <th>confidence_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55075</th>\n",
       "      <td>found transponder snail</td>\n",
       "      <td>0</td>\n",
       "      <td>4145</td>\n",
       "      <td>11365</td>\n",
       "      <td>15510</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>0.022526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267247</td>\n",
       "      <td>0.732753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60288</th>\n",
       "      <td>giants sea monsters</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9710</td>\n",
       "      <td>9710</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97927</th>\n",
       "      <td>monsters amazing encounters</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9710</td>\n",
       "      <td>9710</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128404</th>\n",
       "      <td>sea monsters amazing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9710</td>\n",
       "      <td>9710</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134765</th>\n",
       "      <td>snail giants sea</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9710</td>\n",
       "      <td>9710</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               word  neg_count  neu_count  pos_count  \\\n",
       "55075       found transponder snail          0       4145      11365   \n",
       "60288           giants sea monsters          0          0       9710   \n",
       "97927   monsters amazing encounters          0          0       9710   \n",
       "128404         sea monsters amazing          0          0       9710   \n",
       "134765             snail giants sea          0          0       9710   \n",
       "\n",
       "        total_count  ngram_len  is_ngram  support_neg  support_neu  \\\n",
       "55075         15510          3      True          0.0     0.008216   \n",
       "60288          9710          3      True          0.0     0.000000   \n",
       "97927          9710          3      True          0.0     0.000000   \n",
       "128404         9710          3      True          0.0     0.000000   \n",
       "134765         9710          3      True          0.0     0.000000   \n",
       "\n",
       "        support_pos  confidence_neg  confidence_neu  confidence_pos  \n",
       "55075      0.022526             0.0        0.267247        0.732753  \n",
       "60288      0.019246             0.0        0.000000        1.000000  \n",
       "97927      0.019246             0.0        0.000000        1.000000  \n",
       "128404     0.019246             0.0        0.000000        1.000000  \n",
       "134765     0.019246             0.0        0.000000        1.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_freq.sort_values(by=['ngram_len', 'pos_count'], ascending=[False, False]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bb29f",
   "metadata": {},
   "source": [
    "Visualizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d215de02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dominant_sentiment\n",
       "NEG    15.208770\n",
       "NEU    11.952191\n",
       "POS    12.323765\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA_DF.groupby('dominant_sentiment')['word_count'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3430b",
   "metadata": {},
   "source": [
    "adding columns for later correlation visualizaiton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
