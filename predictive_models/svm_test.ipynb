{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c07a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SVM_implementation import build_models, runner\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7eda470",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_config = {\n",
    "    \"data_path\": \"../project_data/t4sa_data.csv\",  # path to the csv with embeddings\n",
    "}\n",
    "\n",
    "w2v_config = {\n",
    "    \"data_path\"   : \"../project_data/t4sa_data_w2v_sg_test.npz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af45c7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing embeddings...\n",
      "Epoch0: 984/3000(32.80%)\n",
      "Epoch1: 920/3000(30.67%)\n",
      "Epoch2: 1094/3000(36.47%)\n",
      "Epoch3: 1565/3000(52.17%)\n",
      "Epoch4: 1701/3000(56.70%)\n",
      "Epoch5: 1771/3000(59.03%)\n",
      "Epoch6: 1764/3000(58.80%)\n",
      "Epoch7: 1870/3000(62.33%)\n",
      "Epoch8: 1985/3000(66.17%)\n",
      "Epoch9: 2061/3000(68.70%)\n",
      "Epoch10: 2027/3000(67.57%)\n",
      "Epoch11: 2081/3000(69.37%)\n",
      "Epoch12: 2109/3000(70.30%)\n",
      "Epoch13: 2116/3000(70.53%)\n",
      "Epoch14: 2140/3000(71.33%)\n",
      "Epoch15: 2150/3000(71.67%)\n",
      "Epoch16: 2177/3000(72.57%)\n",
      "Epoch17: 2213/3000(73.77%)\n",
      "Epoch18: 2221/3000(74.03%)\n",
      "Epoch19: 2243/3000(74.77%)\n",
      "Epoch20: 2258/3000(75.27%)\n",
      "Epoch21: 2284/3000(76.13%)\n",
      "Epoch22: 2304/3000(76.80%)\n",
      "Epoch23: 2310/3000(77.00%)\n",
      "Epoch24: 2325/3000(77.50%)\n",
      "Epoch25: 2346/3000(78.20%)\n",
      "Epoch26: 2361/3000(78.70%)\n",
      "Epoch27: 2368/3000(78.93%)\n",
      "Epoch28: 2391/3000(79.70%)\n",
      "Epoch29: 2409/3000(80.30%)\n",
      "Epoch30: 2423/3000(80.77%)\n",
      "Epoch31: 2437/3000(81.23%)\n",
      "Epoch32: 2444/3000(81.47%)\n",
      "Epoch33: 2448/3000(81.60%)\n",
      "Epoch34: 2442/3000(81.40%)\n",
      "Epoch35: 2453/3000(81.77%)\n",
      "Epoch36: 2459/3000(81.97%)\n",
      "Epoch37: 2459/3000(81.97%)\n",
      "Epoch38: 2463/3000(82.10%)\n",
      "Epoch39: 2467/3000(82.23%)\n",
      "Epoch40: 2472/3000(82.40%)\n",
      "Epoch41: 2479/3000(82.63%)\n",
      "Epoch42: 2483/3000(82.77%)\n",
      "Epoch43: 2489/3000(82.97%)\n",
      "Epoch44: 2491/3000(83.03%)\n",
      "Epoch45: 2492/3000(83.07%)\n",
      "Epoch46: 2495/3000(83.17%)\n",
      "Epoch47: 2504/3000(83.47%)\n",
      "Epoch48: 2505/3000(83.50%)\n",
      "Epoch49: 2507/3000(83.57%)\n",
      "Epoch50: 2508/3000(83.60%)\n",
      "Epoch51: 2517/3000(83.90%)\n",
      "Epoch52: 2518/3000(83.93%)\n",
      "Epoch53: 2521/3000(84.03%)\n",
      "Epoch54: 2520/3000(84.00%)\n",
      "Epoch55: 2522/3000(84.07%)\n",
      "Epoch56: 2526/3000(84.20%)\n",
      "Epoch57: 2527/3000(84.23%)\n",
      "Epoch58: 2529/3000(84.30%)\n",
      "Epoch59: 2529/3000(84.30%)\n",
      "Epoch60: 2528/3000(84.27%)\n",
      "Epoch61: 2534/3000(84.47%)\n",
      "Epoch62: 2536/3000(84.53%)\n",
      "Epoch63: 2533/3000(84.43%)\n",
      "Epoch64: 2536/3000(84.53%)\n",
      "Epoch65: 2536/3000(84.53%)\n",
      "Epoch66: 2537/3000(84.57%)\n",
      "Epoch67: 2534/3000(84.47%)\n",
      "Epoch68: 2540/3000(84.67%)\n",
      "Epoch69: 2542/3000(84.73%)\n",
      "Epoch70: 2539/3000(84.63%)\n",
      "Epoch71: 2543/3000(84.77%)\n",
      "Epoch72: 2541/3000(84.70%)\n",
      "Epoch73: 2541/3000(84.70%)\n",
      "Epoch74: 2543/3000(84.77%)\n",
      "Epoch75: 2540/3000(84.67%)\n",
      "Epoch76: 2545/3000(84.83%)\n",
      "Epoch77: 2540/3000(84.67%)\n",
      "Epoch78: 2542/3000(84.73%)\n",
      "Epoch79: 2542/3000(84.73%)\n",
      "Epoch80: 2542/3000(84.73%)\n",
      "Epoch81: 2548/3000(84.93%)\n",
      "Epoch82: 2546/3000(84.87%)\n",
      "Epoch83: 2551/3000(85.03%)\n",
      "Epoch84: 2543/3000(84.77%)\n",
      "Epoch85: 2550/3000(85.00%)\n",
      "Epoch86: 2549/3000(84.97%)\n",
      "Epoch87: 2553/3000(85.10%)\n",
      "Epoch88: 2552/3000(85.07%)\n",
      "Epoch89: 2547/3000(84.90%)\n",
      "Epoch90: 2552/3000(85.07%)\n",
      "Epoch91: 2560/3000(85.33%)\n",
      "Epoch92: 2563/3000(85.43%)\n",
      "Epoch93: 2557/3000(85.23%)\n",
      "Epoch94: 2559/3000(85.30%)\n",
      "Epoch95: 2566/3000(85.53%)\n",
      "Epoch96: 2562/3000(85.40%)\n",
      "Epoch97: 2561/3000(85.37%)\n",
      "Epoch98: 2566/3000(85.53%)\n",
      "Epoch99: 2557/3000(85.23%)\n",
      "Epoch100: 2569/3000(85.63%)\n",
      "Epoch101: 2560/3000(85.33%)\n",
      "Epoch102: 2564/3000(85.47%)\n",
      "Epoch103: 2558/3000(85.27%)\n",
      "Epoch104: 2570/3000(85.67%)\n",
      "Epoch105: 2565/3000(85.50%)\n",
      "Epoch106: 2572/3000(85.73%)\n",
      "Epoch107: 2570/3000(85.67%)\n",
      "Epoch108: 2571/3000(85.70%)\n",
      "Epoch109: 2571/3000(85.70%)\n",
      "Epoch110: 2571/3000(85.70%)\n",
      "Epoch111: 2571/3000(85.70%)\n",
      "Epoch112: 2570/3000(85.67%)\n",
      "Epoch113: 2574/3000(85.80%)\n",
      "Epoch114: 2570/3000(85.67%)\n",
      "Epoch115: 2571/3000(85.70%)\n",
      "Epoch116: 2571/3000(85.70%)\n",
      "Epoch117: 2572/3000(85.73%)\n",
      "Epoch118: 2565/3000(85.50%)\n",
      "Epoch119: 2569/3000(85.63%)\n",
      "Epoch120: 2569/3000(85.63%)\n",
      "Epoch121: 2569/3000(85.63%)\n",
      "Epoch122: 2571/3000(85.70%)\n",
      "Early stopping due to no improvement in validation accuracy for 10 epochs\n",
      "Training complete\n",
      "Custom MLP Test Accuracy: 0.8603\n",
      "Scikit-learn SVM Test Accuracy: 0.8720\n"
     ]
    }
   ],
   "source": [
    "# run transformer\n",
    "runner(transformer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453c164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_arr = np.load(w2v_config[\"data_path\"], allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4464ee01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_arr[w2v_arr.files[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e383cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    w2v_arr[w2v_arr.files[0]],\n",
    "    columns = [\"text\", \"class\", \"tokenized_text\", \"embeddings\", \"embeddings_flat\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be54bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list =  [ np.array(l) for l in data[\"embeddings_flat\"]]\n",
    "max_len = max(l.shape[0] for l in embedding_list)\n",
    "data[\"embeddings_flat\"] = [np.pad(l, (0, max_len - l.shape[0])) for l in embedding_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d2373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_runner():\n",
    "    custom_mlp, sk_svm = build_models()\n",
    "    # data = \n",
    "    # data = pd.read_csv(configs[\"data_path\"])\n",
    "\n",
    "\n",
    "    print(\"Parsing embeddings...\")\n",
    "    if isinstance(data['embeddings'].iloc[0], str):\n",
    "        data['embeddings_flat'] = data['embeddings_flat'].str.strip('[]').str.split(',').apply(\n",
    "            lambda x: np.array([float(i) for i in x])\n",
    "        )\n",
    "\n",
    "    X, y = np.stack(data['embeddings_flat'].values), data['class']\n",
    "    # encode the class labels\n",
    "    y_unique = {label: idx for idx, label in enumerate(sorted(y.unique()))}\n",
    "    y = y.map(y_unique).values\n",
    "\n",
    "    # train-test split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # train the custom MLP\n",
    "    custom_mlp.fit(X_train, y_train, X_val, y_val)\n",
    "    acc_mlp = custom_mlp.evaluate((X_test, y_test))\n",
    "    print(f\"Custom MLP Test Accuracy: {acc_mlp:.4f}\")\n",
    "\n",
    "    # train the scikit-learn SVM\n",
    "    sk_svm.fit(X_train, y_train)\n",
    "    acc_svm = sk_svm.score(X_test, y_test)\n",
    "    print(f\"Scikit-learn SVM Test Accuracy on Word2vec embedded text: {acc_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5077adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing embeddings...\n",
      "Epoch0: 6352/10000(63.52%)\n",
      "Epoch1: 6368/10000(63.68%)\n",
      "Epoch2: 6356/10000(63.56%)\n",
      "Epoch3: 6431/10000(64.31%)\n",
      "Epoch4: 6642/10000(66.42%)\n",
      "Epoch5: 6814/10000(68.14%)\n",
      "Epoch6: 7015/10000(70.15%)\n",
      "Epoch7: 7097/10000(70.97%)\n",
      "Epoch8: 7250/10000(72.50%)\n",
      "Epoch9: 7272/10000(72.72%)\n",
      "Epoch10: 7388/10000(73.88%)\n",
      "Epoch11: 7445/10000(74.45%)\n",
      "Epoch12: 7467/10000(74.67%)\n",
      "Epoch13: 7547/10000(75.47%)\n",
      "Epoch14: 7570/10000(75.70%)\n",
      "Epoch15: 7653/10000(76.53%)\n",
      "Epoch16: 7655/10000(76.55%)\n",
      "Epoch17: 7692/10000(76.92%)\n",
      "Epoch18: 7705/10000(77.05%)\n",
      "Epoch19: 7717/10000(77.17%)\n",
      "Epoch20: 7747/10000(77.47%)\n",
      "Epoch21: 7745/10000(77.45%)\n",
      "Epoch22: 7788/10000(77.88%)\n",
      "Epoch23: 7820/10000(78.20%)\n",
      "Epoch24: 7829/10000(78.29%)\n",
      "Epoch25: 7881/10000(78.81%)\n",
      "Epoch26: 7886/10000(78.86%)\n",
      "Epoch27: 7890/10000(78.90%)\n",
      "Epoch28: 7887/10000(78.87%)\n",
      "Epoch29: 7866/10000(78.66%)\n",
      "Epoch30: 7908/10000(79.08%)\n",
      "Epoch31: 7927/10000(79.27%)\n",
      "Epoch32: 7938/10000(79.38%)\n",
      "Epoch33: 7929/10000(79.29%)\n",
      "Epoch34: 7954/10000(79.54%)\n",
      "Epoch35: 7962/10000(79.62%)\n",
      "Epoch36: 7948/10000(79.48%)\n",
      "Epoch37: 7927/10000(79.27%)\n",
      "Epoch38: 7937/10000(79.37%)\n",
      "Epoch39: 7934/10000(79.34%)\n",
      "Epoch40: 7939/10000(79.39%)\n",
      "Epoch41: 7964/10000(79.64%)\n",
      "Epoch42: 7972/10000(79.72%)\n",
      "Epoch43: 7988/10000(79.88%)\n",
      "Epoch44: 7937/10000(79.37%)\n",
      "Epoch45: 7976/10000(79.76%)\n",
      "Epoch46: 7971/10000(79.71%)\n",
      "Epoch47: 8013/10000(80.13%)\n",
      "Epoch48: 7986/10000(79.86%)\n",
      "Epoch49: 8012/10000(80.12%)\n",
      "Epoch50: 8025/10000(80.25%)\n",
      "Epoch51: 7968/10000(79.68%)\n",
      "Epoch52: 8008/10000(80.08%)\n",
      "Epoch53: 8031/10000(80.31%)\n",
      "Epoch54: 7999/10000(79.99%)\n",
      "Epoch55: 7996/10000(79.96%)\n",
      "Epoch56: 7988/10000(79.88%)\n",
      "Epoch57: 8023/10000(80.23%)\n",
      "Epoch58: 8041/10000(80.41%)\n",
      "Epoch59: 7998/10000(79.98%)\n",
      "Epoch60: 8008/10000(80.08%)\n",
      "Epoch61: 8003/10000(80.03%)\n",
      "Epoch62: 8018/10000(80.18%)\n",
      "Epoch63: 8006/10000(80.06%)\n",
      "Epoch64: 7997/10000(79.97%)\n",
      "Epoch65: 8009/10000(80.09%)\n",
      "Epoch66: 8039/10000(80.39%)\n",
      "Epoch67: 8053/10000(80.53%)\n",
      "Epoch68: 8016/10000(80.16%)\n",
      "Epoch69: 8014/10000(80.14%)\n",
      "Epoch70: 8045/10000(80.45%)\n",
      "Epoch71: 8028/10000(80.28%)\n",
      "Epoch72: 8027/10000(80.27%)\n",
      "Epoch73: 8036/10000(80.36%)\n",
      "Epoch74: 8031/10000(80.31%)\n",
      "Epoch75: 8034/10000(80.34%)\n",
      "Epoch76: 8039/10000(80.39%)\n",
      "Epoch77: 8070/10000(80.70%)\n",
      "Epoch78: 8048/10000(80.48%)\n",
      "Epoch79: 8044/10000(80.44%)\n",
      "Epoch80: 8042/10000(80.42%)\n",
      "Epoch81: 8072/10000(80.72%)\n",
      "Epoch82: 8058/10000(80.58%)\n",
      "Epoch83: 8068/10000(80.68%)\n",
      "Epoch84: 8072/10000(80.72%)\n",
      "Epoch85: 8076/10000(80.76%)\n",
      "Epoch86: 8057/10000(80.57%)\n",
      "Epoch87: 8083/10000(80.83%)\n",
      "Epoch88: 8078/10000(80.78%)\n",
      "Epoch89: 8086/10000(80.86%)\n",
      "Epoch90: 8083/10000(80.83%)\n",
      "Epoch91: 8091/10000(80.91%)\n",
      "Epoch92: 8082/10000(80.82%)\n",
      "Epoch93: 8059/10000(80.59%)\n",
      "Epoch94: 8078/10000(80.78%)\n",
      "Epoch95: 8096/10000(80.96%)\n",
      "Epoch96: 8108/10000(81.08%)\n",
      "Epoch97: 8112/10000(81.12%)\n",
      "Epoch98: 8098/10000(80.98%)\n",
      "Epoch99: 8122/10000(81.22%)\n",
      "Epoch100: 8118/10000(81.18%)\n",
      "Epoch101: 8107/10000(81.07%)\n",
      "Epoch102: 8124/10000(81.24%)\n",
      "Epoch103: 8120/10000(81.20%)\n",
      "Epoch104: 8117/10000(81.17%)\n",
      "Epoch105: 8131/10000(81.31%)\n",
      "Epoch106: 8121/10000(81.21%)\n",
      "Epoch107: 8113/10000(81.13%)\n",
      "Epoch108: 8128/10000(81.28%)\n",
      "Epoch109: 8132/10000(81.32%)\n",
      "Epoch110: 8145/10000(81.45%)\n",
      "Epoch111: 8126/10000(81.26%)\n",
      "Epoch112: 8140/10000(81.40%)\n",
      "Epoch113: 8119/10000(81.19%)\n",
      "Epoch114: 8142/10000(81.42%)\n",
      "Epoch115: 8133/10000(81.33%)\n",
      "Epoch116: 8142/10000(81.42%)\n",
      "Epoch117: 8133/10000(81.33%)\n",
      "Epoch118: 8153/10000(81.53%)\n",
      "Epoch119: 8150/10000(81.50%)\n",
      "Epoch120: 8116/10000(81.16%)\n",
      "Epoch121: 8151/10000(81.51%)\n",
      "Epoch122: 8150/10000(81.50%)\n",
      "Epoch123: 8134/10000(81.34%)\n",
      "Epoch124: 8169/10000(81.69%)\n",
      "Epoch125: 8161/10000(81.61%)\n",
      "Epoch126: 8116/10000(81.16%)\n",
      "Epoch127: 8166/10000(81.66%)\n",
      "Epoch128: 8163/10000(81.63%)\n",
      "Epoch129: 8172/10000(81.72%)\n",
      "Epoch130: 8160/10000(81.60%)\n",
      "Epoch131: 8158/10000(81.58%)\n",
      "Epoch132: 8154/10000(81.54%)\n",
      "Epoch133: 8182/10000(81.82%)\n",
      "Epoch134: 8174/10000(81.74%)\n",
      "Epoch135: 8132/10000(81.32%)\n",
      "Epoch136: 8163/10000(81.63%)\n",
      "Epoch137: 8157/10000(81.57%)\n",
      "Epoch138: 8169/10000(81.69%)\n",
      "Epoch139: 8163/10000(81.63%)\n",
      "Epoch140: 8187/10000(81.87%)\n",
      "Epoch141: 8182/10000(81.82%)\n",
      "Epoch142: 8152/10000(81.52%)\n",
      "Epoch143: 8181/10000(81.81%)\n",
      "Epoch144: 8182/10000(81.82%)\n",
      "Epoch145: 8157/10000(81.57%)\n",
      "Epoch146: 8159/10000(81.59%)\n",
      "Epoch147: 8176/10000(81.76%)\n",
      "Epoch148: 8203/10000(82.03%)\n",
      "Epoch149: 8196/10000(81.96%)\n",
      "Training complete\n",
      "Custom MLP Test Accuracy: 0.8152\n",
      "Scikit-learn SVM Test Accuracy on Word2vec embedded text: 0.8240\n"
     ]
    }
   ],
   "source": [
    "w2v_runner()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
